{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from joblib import dump, load\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, Normalizer, Imputer, LabelBinarizer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from analytics_package.cic.preprocessing.preprocessing import *\n",
    "from analytics_package.cic.outlier_processing.outlier_processing import *\n",
    "from analytics_package.cic.pipeline.pipeline import *\n",
    "from analytics_package.cic.io.file import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_types = {\n",
    "    'numeric': [\n",
    "        None\n",
    "    ],\n",
    "    'nominal': [\n",
    "        None\n",
    "    ],\n",
    "    'target': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Model(features_types , model):\n",
    "    pipeline_features_numeric = Pipeline([\n",
    "        ('features', ColumnSelector(features_types['numeric'])),\n",
    "        ('transformer_impute', Imputer()),\n",
    "        ('transformer_norm', Normalizer())\n",
    "    ])\n",
    "\n",
    "    pipeline_features_dummies = Pipeline([\n",
    "        ('features', ColumnSelector(features_types['nominal'])),\n",
    "        ('transformer_impute', FillNaN('0')),\n",
    "        ('transformer_le', MultiColumnLabelEncoder())\n",
    "    ])\n",
    "\n",
    "    pipeline_preprocessing = FeatureUnion([\n",
    "        ('features_numeric', pipeline_features_numeric),\n",
    "        ('features_dummies', pipeline_features_dummies)\n",
    "    ])\n",
    "\n",
    "    model_pipeline = Pipeline([\n",
    "        ('preprocessing', pipeline_preprocessing),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    return model_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    \"model__max_depth\": [5, None],\n",
    "    \"model__max_features\": sp_randint(1, 11),\n",
    "    \"model__min_samples_split\": sp_randint(2, 11),\n",
    "    \"model__criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "model_pipeline = Model(features_types, RandomForestClassifier(max_depth=5))\n",
    "random_search = RandomizedSearchCV(model_pipeline, param_distributions=param_dist, n_iter=20, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = Model(features_types, RandomForestClassifier(max_depth=5))\n",
    "\n",
    "cross_validate(model_pipeline, X, y, cv=3, scoring=['f1_weighted', 'accuracy', 'average_precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame({\n",
    "    'importances': model_pipeline.named_steps['model'].feature_importances_\n",
    "}, index = features_types['numeric'] + features_types['le'] + features_types['binary'], ).sort_values(by='importances')      \n",
    "plt.figure( figsize=(15,8))\n",
    "sns.barplot(x='importances', y='index', data=importances.head(25).reset_index().sort_values(by='importances', ascending=False), color='#bdbdbd')\n",
    "plt.show()\n",
    "\n",
    "x = pd.DataFrame({\n",
    "    'defectors': defection_df[features_types['target']],\n",
    "    'pred': model_pipeline.predict(defection_df),\n",
    "    'proba': model_pipeline.predict_proba(defection_df)[:, 1]\n",
    "})\n",
    "print(classification_report(x['defectors'], x['pred']))\n",
    "_ = sns.heatmap(pd.DataFrame(confusion_matrix(x['defectors'], x['pred']), columns=sorted(x['pred'].unique()), index=sorted(x['pred'].unique())), annot=True, fmt=\"d\", cmap=sns.color_palette('gray' ))\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(x):\n",
    "    fpr, tpr, _ = roc_curve(x['defectors'].ravel(), x['proba'].ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='#fdae61',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='#2c7bb6', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(x):\n",
    "    p, r, _ = precision_recall_curve(x['defectors'], x['proba'])\n",
    "    base = x['defectors'].value_counts()[1] / x.shape[0]\n",
    "    plt.plot(r, p, lw=2, color='#fdae61')\n",
    "    plt.plot([0, 1], [base, base], linestyle='--', lw=2,  color='#2c7bb6')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('P-R Curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(x)\n",
    "plot_roc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_pipeline = Model(features_types, RandomForestClassifier(max_depth=5))\n",
    "model_pipeline.fit(X, y)\n",
    "save_model(model_pipeline, 'output/binarized_models/model_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System modifies environental variables which are read by score_model script\n",
    "os.environ['binary_location'] = \"output/binarized_models/model_pipeline.joblib\"\n",
    "os.environ['file_location'] = \"input/data/data.csv\"\n",
    "os.environ['output_file_location'] = \"output/defection/scores/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean for the purpose of testing properly\n",
    "del model_pipeline\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Productionalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model():\n",
    "    now = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "    \n",
    "    # Load Binarized Pipeline\n",
    "    model_pipeline = load_pickle(os.environ['binary_location'])\n",
    "    \n",
    "    # Load Data from csv\n",
    "    data = pd.read_csv(os.environ['file_location'], delimiter='|')\n",
    "    \n",
    "    #Score and write to a csv file\n",
    "    predictions = model_pipeline.predict(data)\n",
    "    np.savetxt(os.environ['output_file_location'] + now + '.csv', predictions)\n",
    "    \n",
    "    print('Model Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
